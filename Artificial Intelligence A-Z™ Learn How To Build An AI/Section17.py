# Section 17. 

# Annex 1 : Artificial Neural Networks.

'''
Deep Learning ->
Geoffrey Hinton - Godfather of Deep Learning. 
Mimic how the human brain operates. 
Artificial Neural Net. 
Input Layer, Hidden Layer, Output Layer. All connected to each other. 
Shallow Learning. 
Lot and lots of hidden layers - Deep Learning. 

Plan of Attack ->
Neuron, Activation Function, How Neural networks work, How Neural Networks Learn, Gradient Descent, Stochastic Gradient Descent...
...Back-propagation.

Neuron ->
Basic Building Block of Neural Networks. 
Santiago Ram√≥n y Cajal.
Neuron, Dendrites, Axon. Synapse.  
Independent variables - one observation. Standardize them. Make sure they have a mean of zero and variance one, or you want to normalize them. 
Instead of making sure the mean is zero and variance is one, you just subtract the minimum value and you divide by the maximum - minimum...
...so by the range of your values, and therefore you get values between zero and one. Depends on the scenario.
efficient BackProp - Yann LeCun (1998).
Output - Continuous, or binary, categorical(several output values). Whatever inputs you put in that's for one row, output is for same.
Simple Linear Regression, Multivariate Linear Regression. 

'''